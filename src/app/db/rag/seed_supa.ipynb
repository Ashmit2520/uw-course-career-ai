{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "905f5bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cb777e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymupdf # imports the pymupdf library\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def read_document():\n",
    "  reader = pymupdf.open(\"guide.pdf\") # open a document\n",
    "  print(\"Checkpoint 1\")\n",
    "  docs = []\n",
    "  for i, page in enumerate(reader):\n",
    "    if i == 100 or i == 1000:\n",
    "      print(i)\n",
    "    text = page.get_text() # get plain text encoded as UTF-8\n",
    "    docs.append(Document(page_content=text, metadata={\"page\": i}))\n",
    "  \n",
    "  return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32d64521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=5000,\n",
    "        chunk_overlap=500,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803eb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5478\n"
     ]
    }
   ],
   "source": [
    "d = read_document()\n",
    "c = split_text(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "92548b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 1/2169 chunks\n",
      "Inserted 51/2169 chunks\n",
      "Inserted 101/2169 chunks\n",
      "Inserted 151/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 201/2169 chunks\n",
      "Inserted 251/2169 chunks\n",
      "Inserted 301/2169 chunks\n",
      "Inserted 351/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 401/2169 chunks\n",
      "Inserted 451/2169 chunks\n",
      "Inserted 501/2169 chunks\n",
      "Inserted 551/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 601/2169 chunks\n",
      "Inserted 651/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 701/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 751/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 801/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 851/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 901/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 951/2169 chunks\n",
      "Inserted 1001/2169 chunks\n",
      "⚠️ Rate limited, waiting 1 second...\n",
      "Inserted 1051/2169 chunks\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/openai/_base_client.py:1024\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/embeddings'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     content = doc.page_content\n\u001b[32m     27\u001b[39m     metadata = doc.metadata\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     embedding = \u001b[43membedding_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpage_content\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m RateLimitError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m⚠️ Rate limited, waiting 1 second...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:638\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_query\u001b[39m\u001b[34m(self, text, **kwargs)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m, **kwargs: Any) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[32m    629\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[32m    630\u001b[39m \n\u001b[32m    631\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    636\u001b[39m \u001b[33;03m        Embedding for the text.\u001b[39;00m\n\u001b[32m    637\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:590\u001b[39m, in \u001b[36mOpenAIEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    587\u001b[39m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[32m    588\u001b[39m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[32m    589\u001b[39m engine = cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m.deployment)\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    592\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/langchain_openai/embeddings/base.py:478\u001b[39m, in \u001b[36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[39m\u001b[34m(self, texts, engine, chunk_size, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m batched_embeddings: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]] = []\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_kwargs\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    482\u001b[39m         response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/openai/resources/embeddings.py:132\u001b[39m, in \u001b[36mEmbeddings.create\u001b[39m\u001b[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m             embedding.embedding = np.frombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[32m    127\u001b[39m                 base64.b64decode(data), dtype=\u001b[33m\"\u001b[39m\u001b[33mfloat32\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    128\u001b[39m             ).tolist()\n\u001b[32m    130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/embeddings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    135\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/openai/_base_client.py:1256\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1244\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1251\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1252\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1253\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1254\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1255\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/openai/_base_client.py:1030\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1029\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/openai/_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1067\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1068\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "from openai import RateLimitError\n",
    "from supabase import create_client\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "# 1. Connect to Supabase\n",
    "if not os.environ.get(\"SUPABASE_URL\"):\n",
    "    os.environ[\"SUPABASE_URL\"] = getpass.getpass(\"Enter Supabase URL: \")\n",
    "if not os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\"):\n",
    "    os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"] = getpass.getpass(\n",
    "        \"Enter Supabase Service Role Key: \"\n",
    "    )\n",
    "\n",
    "url = os.environ[\"SUPABASE_URL\"]\n",
    "key = os.environ[\"SUPABASE_SERVICE_ROLE_KEY\"]  # Or anon key if permissions allow\n",
    "supabase = create_client(url, key)\n",
    "\n",
    "# 2. Initialize embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 3. Loop over chunks and insert\n",
    "for i, doc in enumerate(c):\n",
    "    try:\n",
    "        content = doc.page_content\n",
    "        metadata = doc.metadata\n",
    "\n",
    "        embedding = embedding_model.embed_query(doc.page_content)\n",
    "    except RateLimitError as e:\n",
    "        print(\"⚠️ Rate limited, waiting 1 second...\")\n",
    "        time.sleep(1)  # or slightly more\n",
    "        # Optional: retry\n",
    "        embedding = embedding_model.embed_query(doc.page_content)\n",
    "\n",
    "        # 4. Insert into Supabase\n",
    "    supabase.table(\"documents\").insert(\n",
    "        {\"content\": content, \"metadata\": metadata, \"embedding\": embedding}\n",
    "    ).execute()\n",
    "\n",
    "    if i % 50 == 0:\n",
    "        print(f\"Inserted {i+1}/{len(c)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f602fa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder './batch_files' created.\n",
      "{\"custom_id\": \"custom_id_1054\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"input\": \"study of politics, economics, security, and culture. The goal is to provide\\nstudents with the necessary tools to understand global processes in their\\ntotality and how they are situated and lived in specific regions.\\nThe major provides an integrated program of courses that lays the\\nfoundation for professional training in a wide variety of areas. Such a\\nfoundation can be invaluable in securing a place in competitive graduate\\nor professional schools, which, in turn, prepare students for government\\nservice, or for other careers with an international focus, including those\\nin multinational corporations, international finance, non-governmental\\norganizations, and institutions of teaching and research.\\nThe International Studies major complements numerous majors across\\ncampus. Many students choose to double major or enhance their studies\", \"model\": \"text-embedding-3-small\", \"encoding_format\": \"float\", \"dimensions\": 1024}}\n",
      "\n",
      "{\"custom_id\": \"custom_id_1055\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"input\": \"2025-2026-fall-undergrad-2 \\n877\\nwith one or more certificates, such as the Global Health certificate or\\nthose offered by the area studies centers.\\nThis major is interdisciplinary, offering a wealth of options. Careful planning\\nand consultation with the International Studies advisor is especially\\nimportant.\\nOPTIONS IN THE MAJOR:\\nHUMAN RIGHTS AND HUMANITARIANISM\\nThis named option covers the range of laws, norms, and organizations\\nthat address the protection of vulnerable populations and ordinary\\ncitizens. This named option covers human rights, as they are codified\\nin the Universal Declaration of Human Rights; hence it covers the civil\\nand political rights, from speech, belief, association, and discrimination,\\nto social, economic, and cultural rights, such as the right to education,\\nhealth, and water, among others. Human Rights and Humanitarianism\\nalso encompasses the organizations and institutions that work to protect\\nand advocate for these rights, inclusive of courses on international law,\\nnonprofit management, and non-state actors in global politics as well as\\nthose on specialized multilateral organizations that deal with human rights,\\nsuch as regional organizations and specialized agencies at the United\\nNations. The named option similarly addresses social movements for\\nchange. It addresses the ways in which civilians are protected when thrust\\ninto conditions of hardship, including the laws of war, regimes on migration\\nand forced migration, and efforts to address global health disparities.\\nGLOBAL SECURITY\\nThis named option covers traditional and non-traditional forms of\\ninsecurity, disorder, and violent conflict as well as efforts to resolve\\nand manage conflict. The thematic categories include forms of war,\\nincluding interstate and civil war, as well as forms of political violence\\nagainst civilians, such as terrorism, genocide, riots, and massacres.\\nGlobal Security also covers non-traditional forms of insecurity, such as\\nclimate change, biothreats, pandemics and other infectious diseases,\\ncyberattacks and cybersecurity, and food security. This named option\\nalso encompasses a range of ways to manage these forms of insecurity,\\nincluding peacekeeping, other multilateral efforts to mediate conflict,\\ncounterinsurgency, and post-conflict peacebuilding.\\u00a0\\nPOLITICS AND POLICY IN THE GLOBAL\\nECONOMY\\nThis named option offers a multidisciplinary survey of international\\neconomic and political institutions and transactions, as well as the policy\\nissues pertaining to international commerce and trade, international\\nfinance and monetary relations, international macroeconomic policy\\ncoordination, U.S. trade imbalances, aid and development, and related\\nenvironmental and natural resource problems.\\nCULTURE IN THE AGE OF GLOBALIZATION\\nIn this named option, students investigate crosscultural interactions at\\ndifferent levels: local, national, and transnational. Students engage in\\nsuch issues as cosmopolitanism; international and global flows of images,\\nideas, and people; questions of identity; changing assumptions of what it\\nmeans to be indigenous and foreign; globalization and technology; and the\\nimpact of globalization on cultures.\\nSTUDY ABROAD\\nInternational Studies and studying abroad are a natural combination. While\\nstudy abroad is not a requirement for the major, all International Studies\\nstudents are strongly encouraged to pursue a significant international\\nexperience during the course of the undergraduate career. Whether\\nthrough a study abroad program, an internship, or service learning,\\nthe experience of studying or working in a foreign culture is invaluable.\\nMany courses taken abroad will count toward the International Studies\\nmajor. See the International Studies advisor for specific guidelines. More\\ninformation about study abroad and internships is available through\\nInternational Academic Programs (http://www.studyabroad.wisc.edu/).\\nHOW TO GET IN\\nHOW TO GET IN\\nStudents are advised to declare the major as soon as they are enrolled in\\nINTL\\u00a0ST\\u00a0101 and/or before studying abroad.\\nTo be eligible to declare the International Studies Major, students must be\\nenrolled or working toward the following courses:\\nCode\\nTitle\\nCredits\\nINTL\\u00a0ST\\u00a0101\\nIntroduction to International Studies\\n3-4\\nREQUIREMENTS\\nUNIVERSITY GENERAL\\nEDUCATION REQUIREMENTS\\nAll undergraduate students at the University of Wisconsin-Madison are\\nrequired to fulfill a minimum set of common\\u00a0university general education\\nrequirements to ensure that every graduate acquires the essential core\\nof an undergraduate education. This core establishes a foundation for\\nliving a productive life, being a citizen of the world, appreciating aesthetic\\nvalues, and engaging in lifelong learning in a continually changing world.\\nVarious schools and colleges will have requirements in addition to the\\nrequirements listed below. Consult your advisor for assistance, as needed.\\nFor additional information, see the university Undergraduate General\\nEducation Requirements (p.\\u00a033) section of the Guide.\", \"model\": \"text-embedding-3-small\", \"encoding_format\": \"float\", \"dimensions\": 1024}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "from httpx import HTTPStatusError\n",
    "from openai import RateLimitError\n",
    "import traceback\n",
    "import json \n",
    "\n",
    "#check if the data folder exists, else create it\n",
    "def check_and_create_folder(folder_path):\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        print(f\"Folder '{folder_path}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_path}' already exists.\")\n",
    "check_and_create_folder('./batch_files')\n",
    "\n",
    "#create the batch files\n",
    "start_index = 1054\n",
    "model_name = \"text-embedding-3-small\"\n",
    "batch_size = 20000\n",
    "batch_content  =  c[start_index:]\n",
    "batch_file_name = 'guide_batch'\n",
    "num_files = len(batch_content) // batch_size + (1 if len(batch_content) % batch_size != 0 else 0)\n",
    "\n",
    "for num_file in range(num_files):\n",
    "    output_file = f'./batch_files/{batch_file_name}_part{num_file}.jsonl'\n",
    "\n",
    "    # make sure that the file does not exist, so don't add to an existing file\n",
    "    if os.path.exists(output_file):\n",
    "        os.remove(output_file)\n",
    "    #write each embedding entry to a new line \n",
    "    with open(output_file, 'a') as file:\n",
    "        for i, doc in enumerate(batch_content[batch_size * num_file : batch_size * (num_file + 1)]): \n",
    "            index = start_index + batch_size * num_file + i\n",
    "        \n",
    "            payload = {\n",
    "                \"custom_id\":f\"custom_id_{index}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/embeddings\",\n",
    "                \"body\": {\n",
    "                    \"input\": doc.page_content,\n",
    "                    \"model\": model_name,\n",
    "                    \"encoding_format\": \"float\",\n",
    "                    'dimensions':1024\n",
    "                }\n",
    "            }\n",
    "            file.write(json.dumps(payload) + '\\n')\n",
    "\n",
    "    # Sanity check, print the first 2 lines\n",
    "    with open(output_file, 'r') as file:\n",
    "        for line in file.readlines()[:2]:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "00d72fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Wrote 557 lines to ./batch_files/split/guide_batch_part0_split0.jsonl\n",
      "✅ Wrote 558 lines to ./batch_files/split/guide_batch_part0_split1.jsonl\n"
     ]
    }
   ],
   "source": [
    "input_path = './batch_files/guide_batch_part0.jsonl'\n",
    "output_dir = './batch_files/split'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read all lines from original JSONL file\n",
    "with open(input_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "# Split in half\n",
    "mid = len(lines) // 2\n",
    "parts = [lines[:mid], lines[mid:]]\n",
    "\n",
    "# Write to new files\n",
    "for i, part in enumerate(parts):\n",
    "    output_path = os.path.join(output_dir, f'guide_batch_part0_split{i}.jsonl')\n",
    "    with open(output_path, 'w') as f:\n",
    "        f.writelines(part)\n",
    "    print(f\"✅ Wrote {len(part)} lines to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d52ed9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📤 Uploading guide_batch_part0_split1.jsonl...\n",
      "📤 Uploading guide_batch_part0_split0.jsonl...\n",
      "🛠️ Creating batch job for guide_batch_part0_split1.jsonl...\n",
      "🛠️ Creating batch job for guide_batch_part0_split0.jsonl...\n",
      "\n",
      "✅ Batch Jobs Created:\n",
      "📦 ID: batch_6872cd80b8c8819091857db32c3a8806 | Status: validating | File: guide_embeddings_part_0\n",
      "📦 ID: batch_6872cd80ff408190b3ada6ce6b43f12d | Status: validating | File: guide_embeddings_part_1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "from openai import OpenAI\n",
    "\n",
    "# Set your OpenAI API key safely\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter OpenAI Key: \")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Folder containing batch JSONL files\n",
    "batch_folder = \"./batch_files\"\n",
    "batch_input_files = []\n",
    "\n",
    "# Upload all batch files to OpenAI\n",
    "for file_name in os.listdir(batch_folder):\n",
    "    file_path = os.path.join(batch_folder, file_name)\n",
    "    if file_name.endswith(\".jsonl\"):\n",
    "        print(f\"📤 Uploading {file_name}...\")\n",
    "        uploaded_file = client.files.create(file=open(file_path, \"rb\"), purpose=\"batch\")\n",
    "        batch_input_files.append(uploaded_file)\n",
    "\n",
    "# Create batch jobs\n",
    "job_creations = []\n",
    "for i, uploaded_file in enumerate(batch_input_files):\n",
    "    print(f\"🛠️ Creating batch job for {uploaded_file.filename}...\")\n",
    "    job = client.batches.create(\n",
    "        input_file_id=uploaded_file.id,\n",
    "        endpoint=\"/v1/embeddings\",\n",
    "        completion_window=\"24h\",  # only valid option currently\n",
    "        metadata={\"description\": f\"guide_embeddings_part_{i}\"},\n",
    "    )\n",
    "    job_creations.append(job)\n",
    "\n",
    "# Print job details\n",
    "print(\"\\n✅ Batch Jobs Created:\")\n",
    "for job in job_creations:\n",
    "    print(\n",
    "        f\"📦 ID: {job.id} | Status: {job.status} | File: {job.metadata['description']}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b141a8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Resubmitted: batch_6872d2f3648c819092435b9a515215dd | Status: validating\n"
     ]
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open('./batch_files/guide_batch_part0_split0.jsonl', 'rb'),  # or split0 path\n",
    "    purpose='batch'\n",
    ")\n",
    "batch_job = client.batches.create(\n",
    "    input_file_id=file.id,\n",
    "    endpoint=\"/v1/embeddings\",\n",
    "    completion_window=\"24h\",\n",
    "    metadata={\n",
    "        \"description\": \"guide_embeddings_part0_split0_resubmit2\"\n",
    "    }\n",
    ")\n",
    "print(f\"✅ Resubmitted: {batch_job.id} | Status: {batch_job.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8452af58",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Batch' object has no attribute 'usage'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[106]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m batches = client.batches.list(limit=\u001b[32m10\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batches:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch.id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch.status\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch.metadata.get(\u001b[33m'\u001b[39m\u001b[33mdescription\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43musage\u001b[49m.input_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Other/SAIL2025/.venv/lib/python3.13/site-packages/pydantic/main.py:991\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m    988\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    990\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Batch' object has no attribute 'usage'"
     ]
    }
   ],
   "source": [
    "batches = client.batches.list(limit=10)\n",
    "for batch in batches:\n",
    "    print(f\"{batch.id} | {batch.status} | {batch.metadata.get('description')} | {batch.usage.input_tokens}\")\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "13803864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544527\n"
     ]
    }
   ],
   "source": [
    "import tiktoken, json\n",
    "\n",
    "enc = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "\n",
    "def estimate_tokens_in_file(path):\n",
    "    total = 0\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            body = json.loads(line)[\"body\"]\n",
    "            total += len(enc.encode(body[\"input\"]))\n",
    "    return total\n",
    "\n",
    "print(estimate_tokens_in_file(\"./batch_files/guide_batch_part0_split0.jsonl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b0435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAIT UNTIL ATLEAST 5pm tomorrow before we can then batch only file guide_batch_part0_split0, part0_split1 is already done."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
